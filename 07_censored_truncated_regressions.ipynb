{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcee6b7",
   "metadata": {},
   "source": [
    "\n",
    "# Censored & Truncated Regressions — Tobit and Truncated Normal — Python Notebook\n",
    "\n",
    "**When to Use**  \n",
    "- The outcome has a **hard boundary** (e.g., sales \\>= 0, minutes watched \\>= 0) and many observations pile up at the limit (**censoring**).  \n",
    "- Data are **observed only above/below a threshold** (e.g., only customers with spend \\> 0 appear), creating **truncation**.\n",
    "\n",
    "**Best Application**  \n",
    "- Purchase incidence or spend with many **zeros** (left‑censored at 0).  \n",
    "- Time‑on‑site, dwell time, loan amounts, bids with limits.  \n",
    "- Sample designs where observations below a cutoff are **not recorded** (truncation).\n",
    "\n",
    "**When Not to Use**  \n",
    "- If zeros are from **two processes** (buy vs. not buy, then how much), consider **two‑part (hurdle)** or **zero‑inflated** models.  \n",
    "- If outcome is **counts**, prefer **Poisson/NegBin** or hurdle count models.\n",
    "\n",
    "**How to Interpret Results**  \n",
    "- **Tobit coefficients** relate to the **latent** outcome. Report **marginal effects** for: (i) unconditional mean of observed y, (ii) probability of being uncensored, (iii) conditional mean given y>c.  \n",
    "- **OLS on censored/truncated data is biased**; use MLE that accounts for censoring/truncation explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bf4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy.stats import norm\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ffe1b",
   "metadata": {},
   "source": [
    "### Data: Simulate latent outcome y* and left‑censor at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2daf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 1500\n",
    "X = np.c_[np.ones(n), rng.normal(0,1,n), rng.normal(0,1,n)]\n",
    "beta_true = np.array([1.0, 2.0, -1.5])\n",
    "sigma_true = 1.0\n",
    "\n",
    "ystar = X @ beta_true + rng.normal(0, sigma_true, n)\n",
    "c = 0.0  # left-censoring at 0\n",
    "y_obs = np.maximum(ystar, c)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['const','x1','x2'])\n",
    "df['y'] = y_obs\n",
    "df['ystar'] = ystar\n",
    "df['censored'] = (y_obs == c).astype(int)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick view: proportion censored\n",
    "df['censored'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(df['y'], bins=40); plt.title('Observed y (left-censored at 0)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422b45e",
   "metadata": {},
   "source": [
    "### Naive OLS on Observed y (Biased under Censoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ols = sm.OLS(df['y'], df[['const','x1','x2']]).fit()\n",
    "ols.params, ols.bse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14f9d4",
   "metadata": {},
   "source": [
    "### Tobit (Left-Censored at 0) via MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540681a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tobit(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog, censor_left=0.0, **kwargs):\n",
    "        super().__init__(endog, exog, **kwargs)\n",
    "        self.c = censor_left\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        beta = params[:-1]\n",
    "        sigma = params[-1]\n",
    "        y = self.endog\n",
    "        X = self.exog\n",
    "        c = self.c\n",
    "\n",
    "        xb = X @ beta\n",
    "        z = (y - xb) / sigma\n",
    "\n",
    "        # Indicator for censored\n",
    "        cens = (y <= c + 1e-12).astype(float)\n",
    "\n",
    "        # For uncensored: log f(y) = log[ (1/sigma) * phi(z) ]\n",
    "        ll_unc = -np.log(sigma) + norm.logpdf(z)\n",
    "\n",
    "        # For censored: log F((c - xb)/sigma)  (probability that y*<=c)\n",
    "        ll_cens = norm.logcdf((c - xb)/sigma)\n",
    "\n",
    "        ll = (1 - cens) * ll_unc + cens * ll_cens\n",
    "        return -ll  # negative log-likelihood per obs\n",
    "\n",
    "    def fit(self, start_params=None, method='bfgs', maxiter=1000, disp=False):\n",
    "        if start_params is None:\n",
    "            # start from OLS on uncensored subset + log(sigma)\n",
    "            mask = self.endog > self.c\n",
    "            beta_ols = np.linalg.lstsq(self.exog[mask], self.endog[mask], rcond=None)[0]\n",
    "            sigma0 = np.std(self.endog[mask] - self.exog[mask] @ beta_ols)\n",
    "            start_params = np.r_[beta_ols, max(sigma0, 0.5)]\n",
    "        return super().fit(start_params=start_params, method=method, maxiter=maxiter, disp=disp)\n",
    "\n",
    "# Fit model\n",
    "endog = df['y'].values\n",
    "exog = df[['const','x1','x2']].values\n",
    "tobit_mod = Tobit(endog, exog, censor_left=0.0)\n",
    "tobit_res = tobit_mod.fit(disp=False)\n",
    "tobit_res.params, tobit_res.bse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d469efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tobit_res.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e29878c",
   "metadata": {},
   "source": [
    "### Marginal Effects (Unconditional mean of observed y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af130229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For left-censored Tobit at c, unconditional E[y] = Phi(a)*(xb) + sigma*phi(a) + c*(1-Phi(a)) where a=(c-xb)/sigma\n",
    "# We want dE[y]/dX = Phi(a)*beta + derivative through a terms.\n",
    "# Closed-forms exist; here's a vectorized function for marginal effects at sample means.\n",
    "\n",
    "beta = tobit_res.params[:-1]\n",
    "sigma = tobit_res.params[-1]\n",
    "xb = df[['const','x1','x2']].mean().values @ beta\n",
    "a = (0.0 - xb) / sigma\n",
    "\n",
    "Phi = norm.cdf(a)\n",
    "phi = norm.pdf(a)\n",
    "\n",
    "# dE[y]/dX ≈ Phi * beta   (common practical approximation for quick guidance)\n",
    "me_approx = Phi * beta\n",
    "pd.Series(me_approx, index=['const','x1','x2']).to_frame('ME_uncond_approx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93796d",
   "metadata": {},
   "source": [
    "### Truncated Normal Regression (observe only y>0) via MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ac61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build truncated sample: drop y==0 (only keep observed y>0)\n",
    "df_trunc = df[df['y'] > 0].copy()\n",
    "\n",
    "class TruncatedReg(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog, trunc_left=0.0, **kwargs):\n",
    "        super().__init__(endog, exog, **kwargs)\n",
    "        self.c = trunc_left\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        beta = params[:-1]\n",
    "        sigma = params[-1]\n",
    "        y = self.endog\n",
    "        X = self.exog\n",
    "        c = self.c\n",
    "        xb = X @ beta\n",
    "        z = (y - xb)/sigma\n",
    "        a = (c - xb)/sigma\n",
    "\n",
    "        # Truncated normal density: f_T(y) = f(y) / (1 - Phi(a))\n",
    "        # log f_T = log f(y) - log(1-Phi(a))\n",
    "        ll = -np.log(sigma) + norm.logpdf(z) - np.log(1 - norm.cdf(a) + 1e-12)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, start_params=None, method='bfgs', maxiter=1000, disp=False):\n",
    "        if start_params is None:\n",
    "            beta_ols = np.linalg.lstsq(self.exog, self.endog, rcond=None)[0]\n",
    "            sigma0 = np.std(self.endog - self.exog @ beta_ols)\n",
    "            start_params = np.r_[beta_ols, max(sigma0, 0.5)]\n",
    "        return super().fit(start_params=start_params, method=method, maxiter=maxiter, disp=disp)\n",
    "\n",
    "# Fit truncated regression\n",
    "endog_tr = df_trunc['y'].values\n",
    "exog_tr  = df_trunc[['const','x1','x2']].values\n",
    "tr_mod = TruncatedReg(endog_tr, exog_tr, trunc_left=0.0)\n",
    "tr_res = tr_mod.fit(disp=False)\n",
    "print(tr_res.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b368d8",
   "metadata": {},
   "source": [
    "### Compare Estimates: True β vs OLS vs Tobit vs Truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comp = pd.DataFrame({\n",
    "    'true_beta': [1.0, 2.0, -1.5],\n",
    "    'OLS_censored': ols.params.values,\n",
    "    'Tobit': tobit_res.params[:-1],\n",
    "    'TruncatedReg': tr_res.params[:-1]\n",
    "}, index=['const','x1','x2']).round(3)\n",
    "\n",
    "comp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132de28",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Practical Guidance\n",
    "- Inspect the **mass at the censoring point**; if large, Tobit often outperforms OLS.  \n",
    "- For purchase data, consider **two‑part models**: (i) logit for incidence, (ii) GLM (e.g., log‑normal/Gamma) for positive spend.  \n",
    "- With **heteroskedasticity** or non‑normal errors, classical Tobit can be misspecified; consider **semiparametric** alternatives.\n",
    "\n",
    "### References (non‑link citations)\n",
    "1. Greene — *Econometric Analysis*.  \n",
    "2. Wooldridge — *Econometric Analysis of Cross Section and Panel Data*.  \n",
    "3. Amemiya — *Tobit Models: A Survey*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
