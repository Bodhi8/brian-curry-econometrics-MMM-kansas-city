{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11b9378",
   "metadata": {},
   "source": [
    "\n",
    "# Latent Class Regression (Finite Mixture of Regressions) — Python Notebook\n",
    "\n",
    "**When to Use**  \n",
    "- Your data are a **mixture of subpopulations** (segments) with **different regression relationships** (e.g., price‑sensitive vs brand‑loyal customers).  \n",
    "- You want to **discover unobserved classes** and estimate a separate regression for each class while learning **class membership probabilities**.\n",
    "\n",
    "**Best Application**  \n",
    "- **Segmentation with outcomes** (e.g., spend, CLV) where segments have distinct slopes/intercepts.  \n",
    "- Marketing response modeling where treatment effects vary by **unobserved groups**.  \n",
    "- As a precursor to **targeting** (assign customers to the most likely class).\n",
    "\n",
    "**When Not to Use**  \n",
    "- If differences are purely **continuous heterogeneity** (not clustered), prefer **hierarchical/mixed models**.  \n",
    "- If class count is very large or unstable and you need **individual‑level draws**, consider **HB (random coefficients/mixed logit)**.\n",
    "\n",
    "**How to Interpret Results**  \n",
    "- Each class has its own **regression coefficients** and **variance**.  \n",
    "- **Mixing weights** indicate the prevalence of each segment.  \n",
    "- **Responsibilities** (posterior class probabilities) provide **soft assignments** per observation; use them to profile segments.  \n",
    "- Use **BIC**/**AIC** to select the number of classes K; validate stability across seeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import inv, slogdet\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "rng = np.random.default_rng(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846652c5",
   "metadata": {},
   "source": [
    "### Data: Synthetic segments with distinct regression slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 1200\n",
    "# True segments (K=3)\n",
    "z_true = rng.choice([0,1,2], size=n, p=[0.45, 0.35, 0.20])\n",
    "\n",
    "X1 = rng.normal(0, 1, size=n)\n",
    "X2 = rng.normal(0, 1, size=n)\n",
    "X = np.c_[np.ones(n), X1, X2]  # intercept + two features\n",
    "\n",
    "# Class-specific betas and sigmas\n",
    "betas = np.array([\n",
    "    [5.0,  2.0,  0.0],   # class 0: strong X1\n",
    "    [3.0, -1.0,  2.5],   # class 1: negative X1, strong X2\n",
    "    [1.5,  0.0, -2.0],   # class 2: strong negative X2\n",
    "])\n",
    "sigmas = np.array([1.0, 1.3, 0.8])\n",
    "\n",
    "y = np.array([X[i] @ betas[z_true[i]] + rng.normal(0, sigmas[z_true[i]]) for i in range(n)])\n",
    "\n",
    "df = pd.DataFrame({'y': y, 'x1': X1, 'x2': X2, 'true_class': z_true})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b30f9",
   "metadata": {},
   "source": [
    "### EM Algorithm for Finite Mixture of Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd04ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def em_mixture_regression(y, X, K=2, max_iter=200, tol=1e-6, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n, p = X.shape\n",
    "\n",
    "    # Initialize responsibilities randomly\n",
    "    resp = rng.dirichlet(alpha=np.ones(K), size=n)  # n x K\n",
    "\n",
    "    # Parameter containers\n",
    "    pis = np.ones(K) / K\n",
    "    betas = np.zeros((K, p))\n",
    "    sig2 = np.ones(K)\n",
    "\n",
    "    ll_history = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # M-step\n",
    "        Nk = resp.sum(axis=0) + 1e-12\n",
    "        pis = Nk / n\n",
    "\n",
    "        for k in range(K):\n",
    "            W = np.diag(resp[:,k])\n",
    "            XtW = X.T @ W\n",
    "            beta_k = inv(XtW @ X + 1e-8*np.eye(p)) @ (XtW @ y)\n",
    "            betas[k] = beta_k\n",
    "            resid = y - X @ beta_k\n",
    "            sig2[k] = (resp[:,k] * resid**2).sum() / Nk[k]\n",
    "\n",
    "        # E-step: update responsibilities\n",
    "        dens = np.zeros((n, K))\n",
    "        for k in range(K):\n",
    "            resid = y - X @ betas[k]\n",
    "            dens[:,k] = pis[k] * (1/np.sqrt(2*np.pi*sig2[k])) * np.exp(-0.5 * (resid**2) / sig2[k])\n",
    "\n",
    "        # avoid zeros\n",
    "        dens = np.clip(dens, 1e-300, None)\n",
    "        ll = np.sum(np.log(dens.sum(axis=1)))\n",
    "        ll_history.append(ll)\n",
    "\n",
    "        resp = dens / dens.sum(axis=1, keepdims=True)\n",
    "\n",
    "        if it > 0 and abs(ll_history[-1] - ll_history[-2]) < tol:\n",
    "            break\n",
    "\n",
    "    # Compute BIC = -2*LL + k*log(n); parameters per class: p betas + 1 variance + 1 mixing - but sum pi=1 => (K-1) free pis\n",
    "    n_params = K * (p + 1) + (K - 1)\n",
    "    bic = -2*ll_history[-1] + n_params * np.log(n)\n",
    "    return {\n",
    "        'pis': pis, 'betas': betas, 'sig2': sig2,\n",
    "        'resp': resp, 'loglike': ll_history[-1], 'bic': bic, 'll_hist': ll_history\n",
    "    }\n",
    "\n",
    "Xmat = np.c_[np.ones(len(df)), df['x1'].values, df['x2'].values]\n",
    "yvec = df['y'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ba694",
   "metadata": {},
   "source": [
    "### Fit Models with K=2 and K=3 Classes; Select via BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res2 = em_mixture_regression(yvec, Xmat, K=2, seed=7)\n",
    "res3 = em_mixture_regression(yvec, Xmat, K=3, seed=7)\n",
    "\n",
    "comp = pd.DataFrame({\n",
    "    'K': [2,3],\n",
    "    'loglike': [res2['loglike'], res3['loglike']],\n",
    "    'BIC': [res2['bic'], res3['bic']]\n",
    "})\n",
    "comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best = res3 if res3['bic'] < res2['bic'] else res2\n",
    "K = 3 if best is res3 else 2\n",
    "K, best['pis'], best['betas'], best['sig2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c411b68",
   "metadata": {},
   "source": [
    "### Posterior Class Probabilities (Responsibilities) and Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resp = best['resp']\n",
    "assign = resp.argmax(axis=1)\n",
    "df['class_hat'] = assign\n",
    "for k in range(K):\n",
    "    df[f'resp_{k}'] = resp[:,k]\n",
    "\n",
    "df[['class_hat','resp_0']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a478f7",
   "metadata": {},
   "source": [
    "### Segment Profiles and Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pis = best['pis']\n",
    "betas = best['betas']\n",
    "sig2 = best['sig2']\n",
    "\n",
    "coef_table = []\n",
    "for k in range(K):\n",
    "    coef_table.append({\n",
    "        'class': k,\n",
    "        'mix_weight': pis[k],\n",
    "        'intercept': betas[k,0],\n",
    "        'beta_x1': betas[k,1],\n",
    "        'beta_x2': betas[k,2],\n",
    "        'sigma': np.sqrt(sig2[k])\n",
    "    })\n",
    "coef_df = pd.DataFrame(coef_table).round(4)\n",
    "coef_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Profile by means of x1/x2 and y within each assigned class\n",
    "profile = df.groupby('class_hat')[['x1','x2','y']].mean().rename(columns=lambda c: c+'_mean')\n",
    "pd.concat([coef_df.set_index('class'), profile], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dd862",
   "metadata": {},
   "source": [
    "### Visualization: Scatter with Class Colors and Fitted Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = np.array(['tab:blue','tab:orange','tab:green','tab:red','tab:purple'])\n",
    "cmap = colors[assign]\n",
    "\n",
    "plt.scatter(df['x1'], df['y'], c=cmap, alpha=0.4, s=15)\n",
    "xline = np.linspace(df['x1'].min(), df['x1'].max(), 100)\n",
    "Xline = np.c_[np.ones_like(xline), xline, np.full_like(xline, df['x2'].mean())]\n",
    "\n",
    "for k in range(K):\n",
    "    yk = Xline @ betas[k]\n",
    "    plt.plot(xline, yk, label=f'class {k} fit')\n",
    "\n",
    "plt.xlabel('x1'); plt.ylabel('y')\n",
    "plt.title('Latent Class Regression: y vs x1 (lines at avg x2)')\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb0f9e",
   "metadata": {},
   "source": [
    "### Predict for New Observations (mixture expectation and classwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_mixture(Xnew, res):\n",
    "    pis, betas = res['pis'], res['betas']\n",
    "    yk = Xnew @ betas.T  # n_new x K\n",
    "    # Mixture conditional expectation (ignore noise)\n",
    "    return (yk * pis).sum(axis=1), yk\n",
    "\n",
    "Xnew = np.c_[np.ones(5), np.linspace(-1.5, 1.5, 5), np.linspace(-1.0, 1.0, 5)]\n",
    "y_mix, y_by_class = predict_mixture(Xnew, best)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'x1': Xnew[:,1], 'x2': Xnew[:,2], 'y_mixture_pred': np.round(y_mix,3),\n",
    "    **{f'y_class{k}': np.round(y_by_class[:,k],3) for k in range(K)}\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c3d41",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Practical Guidance\n",
    "- Run EM with **multiple random seeds**; choose the best (highest loglike / lowest BIC).  \n",
    "- Standardize features before modeling when scales differ; consider **regularization** for stability.  \n",
    "- Use **soft assignments** (responsibilities) to avoid hard thresholding when targeting.  \n",
    "- Compare against **HB/random‑coefficient** models to check whether heterogeneity is continuous rather than clustered.\n",
    "\n",
    "### References (non‑link citations)\n",
    "1. Wedel & Kamakura — *Market Segmentation: Conceptual and Methodological Foundations*.  \n",
    "2. McLachlan & Peel — *Finite Mixture Models*.  \n",
    "3. Rossi, Allenby & McCulloch — *Bayesian Statistics and Marketing*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
