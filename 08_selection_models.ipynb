{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b736017d",
   "metadata": {},
   "source": [
    "\n",
    "# Selection Models & Non‑Random Treatment Effects — Heckman + Propensity Scores — Python Notebook\n",
    "\n",
    "**When to Use**  \n",
    "- Treatment/exposure is **not randomized** (e.g., targeted ads shown to likely buyers), or your outcome is observed only for a **selected sample** (e.g., spend observed only for purchasers).  \n",
    "- Want to correct for **selection bias** or estimate **causal lift** using observational data.\n",
    "\n",
    "**Best Application**  \n",
    "- **Sample selection**: two‑step Heckman when the decision to be observed is modeled separately (e.g., purchase vs. spend).  \n",
    "- **Propensity scores**: IPW or matching when you can defend **selection on observables**.  \n",
    "- Combine with business rules and **experiments** for validation.\n",
    "\n",
    "**When Not to Use**  \n",
    "- When **strong instruments** exist → prefer **IV / RCT / geo‑experiments**.  \n",
    "- If unobserved confounding is large and unaddressed, PS methods will still be biased.\n",
    "\n",
    "**How to Interpret Results**  \n",
    "- **Heckman**: the **inverse Mills ratio (IMR)** captures selection; a significant IMR implies non‑random selection.  \n",
    "- **PSM/IPW**: effects are **conditional on assumptions** (no hidden bias). Always perform **balance checks** and **sensitivity analyses**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "pd.set_option('display.max_columns', 160)\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "rng = np.random.default_rng(222)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d936a",
   "metadata": {},
   "source": [
    "### Data: Simulate purchase selection and ad exposure with confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 5000\n",
    "# Covariates\n",
    "x1 = rng.normal(0,1,n)                 # e.g., prior engagement\n",
    "x2 = rng.normal(0,1,n)                 # e.g., income proxy\n",
    "u  = rng.normal(0,1,n)                 # unobserved taste\n",
    "\n",
    "# Treatment (ad exposure) depends on x1 and unobserved u (endogeneity)\n",
    "p_treat = 1/(1+np.exp(-(0.5*x1 + 0.3*x2 + 0.6*u)))\n",
    "treat = rng.binomial(1, p_treat)\n",
    "\n",
    "# Selection: purchase indicator depends on x1, treat, and u\n",
    "p_buy = 1/(1+np.exp(-(-0.3 + 0.8*x1 + 0.4*treat + 0.7*u)))\n",
    "buy = rng.binomial(1, p_buy)\n",
    "\n",
    "# Outcome only observed if buy==1 (e.g., spend conditional on purchase)\n",
    "eps = rng.normal(0, 1, n)\n",
    "spend_star = 10 + 2.0*x1 + 1.2*x2 + 1.5*treat + 0.8*u + eps  # latent spend\n",
    "spend = np.where(buy==1, spend_star, np.nan)\n",
    "\n",
    "df = pd.DataFrame({'x1':x1,'x2':x2,'treat':treat,'buy':buy,'spend':spend})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e9139",
   "metadata": {},
   "source": [
    "### Naive Estimates (biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Naive treatment effect on spend ignoring selection (use observed spend only)\n",
    "naive_ols = smf.ols(\"spend ~ treat + x1 + x2\", data=df[df['buy']==1]).fit()\n",
    "naive_ate = df.loc[df['buy']==1].groupby('treat')['spend'].mean().diff().iloc[-1]\n",
    "{'naive_ols_coef_treat': naive_ols.params['treat'], 'group_diff_observed': naive_ate}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7e500",
   "metadata": {},
   "source": [
    "### Heckman Two‑Step Sample Selection Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43451b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Probit selection (buy) using variables that affect selection; include an exclusion restriction if available\n",
    "# We'll assume x1 and x2 affect both, but add z that affects selection only (exclusion)\n",
    "z = (df['x2'] > 0).astype(int)  # proxy instrument for selection (toy)\n",
    "df['z'] = z\n",
    "\n",
    "probit = smf.probit(\"buy ~ x1 + x2 + treat + z\", data=df.assign(buy=df['buy'].astype(float))).fit(disp=False)\n",
    "df['XB'] = probit.predict(linear=True)\n",
    "df['lambda'] = norm.pdf(df['XB']) / norm.cdf(df['XB'])  # IMR for buy==1; we use only on observed spend rows\n",
    "\n",
    "# Step 2: Outcome on observed sample with IMR\n",
    "heckman = smf.ols(\"spend ~ treat + x1 + x2 + lambda\", data=df[df['buy']==1]).fit()\n",
    "heckman.params, heckman.bse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(heckman.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67314e8",
   "metadata": {},
   "source": [
    "**Interpretation:** Significant `lambda` suggests selection bias. The `treat` coefficient is corrected for selection under model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb95d8",
   "metadata": {},
   "source": [
    "### Propensity Scores (PS) and Balance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc883f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estimate PS using observables (exclude u by design)\n",
    "X = df[['x1','x2']].values\n",
    "ps_model = LogisticRegression(max_iter=500).fit(X, df['treat'])\n",
    "df['ps'] = ps_model.predict_proba(X)[:,1]\n",
    "\n",
    "def std_diff(a, b):\n",
    "    mu1, mu0 = a.mean(), b.mean()\n",
    "    s = np.sqrt(0.5*(a.var()+b.var()) + 1e-9)\n",
    "    return (mu1 - mu0)/s\n",
    "\n",
    "balance_before = {\n",
    "    'x1': std_diff(df.loc[df.treat==1,'x1'], df.loc[df.treat==0,'x1']),\n",
    "    'x2': std_diff(df.loc[df.treat==1,'x2'], df.loc[df.treat==0,'x2']),\n",
    "    'ps': std_diff(df.loc[df.treat==1,'ps'], df.loc[df.treat==0,'ps']),\n",
    "}\n",
    "balance_before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e07466",
   "metadata": {},
   "source": [
    "### IPW: Inverse Probability Weighting for ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use only rows with spend observed (buy==1) for ATT on purchasers, or model missingness; here we'll estimate ATE on spend_star proxy\n",
    "# For demonstration, compute ATE on latent spend_star (oracle) and compare with IPW on observed spend (approx)\n",
    "df['spend_star'] = df['spend'].where(df['buy']==1, np.nan)  # keep NaN where not observed to emphasize selection\n",
    "\n",
    "# IPW weights\n",
    "e = df['ps'].clip(1e-3, 1-1e-3)\n",
    "w = np.where(df['treat']==1, 1/e, 1/(1-e))\n",
    "\n",
    "# Use observed spend (will be biased if many NaNs); drop NaNs\n",
    "ipw_df = df.dropna(subset=['spend']).copy()\n",
    "e_obs = ipw_df['ps'].clip(1e-3, 1-1e-3)\n",
    "w_obs = np.where(ipw_df['treat']==1, 1/e_obs, 1/(1-e_obs))\n",
    "\n",
    "ipw_ate = (w_obs * (ipw_df['treat']*ipw_df['spend'] / e_obs - (1-ipw_df['treat'])*ipw_df['spend'] / (1-e_obs))).mean()\n",
    "ipw_ate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e31b15",
   "metadata": {},
   "source": [
    "### Propensity Score Matching (Nearest Neighbor, 1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622fa10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m_df = df.dropna(subset=['spend']).copy()\n",
    "treated = m_df[m_df['treat']==1].copy()\n",
    "control = m_df[m_df['treat']==0].copy()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=1).fit(control[['ps']].values)\n",
    "dist, idx = nbrs.kneighbors(treated[['ps']].values)\n",
    "matched_controls = control.iloc[idx.flatten()].copy()\n",
    "matched_controls.index = treated.index  # align\n",
    "\n",
    "att_psm = (treated['spend'].values - matched_controls['spend'].values).mean()\n",
    "att_psm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56bcff",
   "metadata": {},
   "source": [
    "### Comparison of Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e23495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comp = pd.Series({\n",
    "    'Naive_OLS_observed_treat_coef': naive_ols.params['treat'],\n",
    "    'Heckman_treat_coef': heckman.params['treat'],\n",
    "    'IPW_ATE_on_observed': ipw_ate,\n",
    "    'PSM_ATT_on_observed': att_psm\n",
    "}).to_frame('estimate')\n",
    "comp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ab9fe",
   "metadata": {},
   "source": [
    "### Balance Check After Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matched = pd.concat([treated[['x1','x2','ps']], matched_controls[['x1','x2','ps']].rename(columns=lambda c: c+\"_c\")], axis=1)\n",
    "\n",
    "balance_after = {\n",
    "    'x1': std_diff(treated['x1'], matched_controls['x1']),\n",
    "    'x2': std_diff(treated['x2'], matched_controls['x2']),\n",
    "    'ps': std_diff(treated['ps'], matched_controls['ps']),\n",
    "}\n",
    "balance_before, balance_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3d34e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Practical Guidance\n",
    "- Include an **exclusion restriction** in Heckman (a variable that affects selection, not the outcome).  \n",
    "- With propensity scores, check **overlap** (0.1–0.9) and improve balance (calipers, stratification, weighting).  \n",
    "- Validate with **experiments** when possible; report **assumption checks** and **sensitivity** (e.g., Rosenbaum bounds).  \n",
    "- If a strong instrument exists, prefer **IV/2SLS**.\n",
    "\n",
    "### References (non‑link citations)\n",
    "1. Heckman — *Sample Selection Bias as a Specification Error*.  \n",
    "2. Rosenbaum & Rubin — *The Central Role of the Propensity Score*.  \n",
    "3. Angrist & Pischke — *Mostly Harmless Econometrics*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
