{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a90340a",
   "metadata": {},
   "source": [
    "\n",
    "# Multi‑Touch Attribution (MTA) — Python Notebook\n",
    "\n",
    "**When to Use**  \n",
    "- You have **user‑level journey data** (touchpoints over time) and want to allocate credit for conversions across channels.  \n",
    "- Need **tactical optimization** (bids, creatives, audiences) and near‑real‑time feedback loops.\n",
    "\n",
    "**Best Application**  \n",
    "- Digital media with identifiable touchpoints: **Search, Social, Display, Email**, SMS, Push.  \n",
    "- Evaluating **paths** (e.g., Paid Social → Search → Direct) and **sequences** (order effects).  \n",
    "- Complementary to MMM: MTA for short‑term granularity; MMM for long‑term budgeting.\n",
    "\n",
    "**When Not to Use**  \n",
    "- When **user‑level tracking is unavailable** (privacy, ATT, cookie deprecation) → prefer MMM or **experiments/geo‑tests**.  \n",
    "- When **incrementality** is required and selection bias is strong → prefer **causal lift tests** (RCT/geo holdouts) or IV/PSM.\n",
    "\n",
    "**How to Interpret Results**  \n",
    "- **Heuristic attributions** (last‑touch, time‑decay) are **rules**, not estimates — use for ops baselines only.  \n",
    "- **Logistic regression** attributions use coefficients/margins as **associational** signals — validate with experiments.  \n",
    "- **Markov path removal effect** estimates each channel’s **path contribution**; still non‑causal absent randomization.  \n",
    "- Always triangulate MTA with **lift experiments** and **MMM** before making large budget moves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb5020",
   "metadata": {},
   "source": [
    "### Data: Synthetic user‑journey paths with channels and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa42c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "channels = [\"Paid_Social\",\"Search\",\"Display\",\"Email\",\"Affiliate\",\"Direct\"]\n",
    "n_users = 600\n",
    "\n",
    "# Generate journeys: each user gets 1-6 touches; conversion prob depends on touches & presence of Search/Email\n",
    "rows = []\n",
    "user_id = 0\n",
    "for u in range(n_users):\n",
    "    k = rng.integers(1, 7)\n",
    "    # sample a path with some bias\n",
    "    path = list(rng.choice(channels, size=k, replace=True, p=[0.2,0.25,0.2,0.15,0.1,0.1]))\n",
    "    # conversion probability based on presence and recency of Search/Email\n",
    "    base = 0.05 + 0.02*len(path)\n",
    "    if \"Search\" in path: base += 0.10\n",
    "    if \"Email\" in path: base += 0.06\n",
    "    if path[-1] == \"Search\": base += 0.05  # last-touch boost\n",
    "    p_conv = np.clip(base, 0, 0.9)\n",
    "    converted = (rng.random() < p_conv)\n",
    "    t0 = rng.integers(1, 30)\n",
    "    for i, ch in enumerate(path):\n",
    "        rows.append({\n",
    "            \"user_id\": user_id,\n",
    "            \"touch_idx\": i+1,\n",
    "            \"channel\": ch,\n",
    "            \"ts\": t0 + i,\n",
    "            \"converted\": int(converted),\n",
    "        })\n",
    "    user_id += 1\n",
    "\n",
    "journeys = pd.DataFrame(rows).sort_values([\"user_id\",\"touch_idx\"]).reset_index(drop=True)\n",
    "journeys.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703fbaf",
   "metadata": {},
   "source": [
    "### Baselines: Last‑Touch and Time‑Decay Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Last-touch: assign full credit to the last channel in the path for converters only\n",
    "last_touches = journeys.loc[journeys.groupby('user_id')['touch_idx'].idxmax()][['user_id','channel','converted']]\n",
    "lt_credit = last_touches[last_touches['converted']==1]['channel'].value_counts().rename('last_touch_credit')\n",
    "\n",
    "# Time-decay: exponential weight by recency for converters\n",
    "decay_lambda = 0.6\n",
    "td_credit = Counter()\n",
    "for uid, grp in journeys.groupby('user_id'):\n",
    "    if grp['converted'].max()==1:\n",
    "        weights = np.exp(decay_lambda * (grp['touch_idx'] - grp['touch_idx'].max()))\n",
    "        weights = weights / weights.sum()\n",
    "        for ch, w in zip(grp['channel'], weights):\n",
    "            td_credit[ch] += w\n",
    "td_credit = pd.Series(td_credit, name='time_decay_credit').sort_values(ascending=False)\n",
    "\n",
    "baseline = pd.concat([lt_credit, td_credit], axis=1).fillna(0).sort_values('time_decay_credit', ascending=False)\n",
    "baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278b0b1",
   "metadata": {},
   "source": [
    "### Logistic Regression MTA: Touch indicators + simple sequence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5011e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build per-user features: counts per channel, last channel, first channel, path length\n",
    "def build_user_features(df):\n",
    "    feats = []\n",
    "    for uid, grp in df.groupby('user_id'):\n",
    "        d = {'user_id': uid, 'converted': int(grp['converted'].max())}\n",
    "        counts = grp['channel'].value_counts()\n",
    "        for ch in channels:\n",
    "            d[f'count_{ch}'] = int(counts.get(ch, 0))\n",
    "        d['first_channel'] = grp.iloc[0]['channel']\n",
    "        d['last_channel'] = grp.iloc[-1]['channel']\n",
    "        d['path_len'] = int(grp['touch_idx'].max())\n",
    "        feats.append(d)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "users = build_user_features(journeys)\n",
    "X = users.drop(columns=['user_id','converted'])\n",
    "y = users['converted']\n",
    "\n",
    "categoricals = ['first_channel','last_channel']\n",
    "numerics = [c for c in X.columns if c not in categoricals]\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categoricals),\n",
    "    ('num', 'passthrough', numerics)\n",
    "])\n",
    "\n",
    "logit = LogisticRegression(max_iter=200, penalty='l2', solver='lbfgs')\n",
    "\n",
    "pipe = Pipeline([('prep', preprocess), ('clf', logit)])\n",
    "pipe.fit(X, y)\n",
    "\n",
    "proba = pipe.predict_proba(X)[:,1]\n",
    "auc = roc_auc_score(y, proba)\n",
    "print(f\"AUC: {auc:.3f}\")\n",
    "\n",
    "print(classification_report(y, pipe.predict(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13f0e7",
   "metadata": {},
   "source": [
    "### Channel Importance from Model Coefficients (Associational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5326c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map model coefficients back to human-readable feature names\n",
    "ohe = pipe.named_steps['prep'].named_transformers_['cat']\n",
    "cat_names = list(ohe.get_feature_names_out(categoricals))\n",
    "feat_names = cat_names + numerics\n",
    "\n",
    "coef = pipe.named_steps['clf'].coef_[0]\n",
    "importance = pd.Series(coef, index=feat_names).sort_values(ascending=False)\n",
    "\n",
    "# Aggregate importance across: counts per channel + first/last indicators for that channel\n",
    "channel_scores = {ch:0.0 for ch in channels}\n",
    "for name, val in importance.items():\n",
    "    for ch in channels:\n",
    "        if name.endswith(ch) or name == f'count_{ch}':\n",
    "            channel_scores[ch] += val\n",
    "\n",
    "channel_scores = pd.Series(channel_scores).sort_values(ascending=False).rename('coef_score')\n",
    "channel_scores.to_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6b222",
   "metadata": {},
   "source": [
    "### Markov Path Attribution: Removal Effect per Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68960a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build transition probabilities between start->channel->...->convert/null states\n",
    "START, CONV, NULL = \"_start_\", \"_conv_\", \"_null_\"\n",
    "def build_paths(df):\n",
    "    paths = []\n",
    "    for uid, grp in df.groupby('user_id'):\n",
    "        chain = [START] + list(grp['channel'].values)\n",
    "        chain.append(CONV if grp['converted'].max()==1 else NULL)\n",
    "        paths.append(chain)\n",
    "    return paths\n",
    "\n",
    "def transition_matrix(paths):\n",
    "    counts = defaultdict(Counter)\n",
    "    for path in paths:\n",
    "        for a, b in zip(path[:-1], path[1:]):\n",
    "            counts[a][b] += 1\n",
    "    # normalize\n",
    "    trans = {a: {b: c/sum(bs.values()) for b, c in bs.items()} for a, bs in counts.items()}\n",
    "    return trans\n",
    "\n",
    "def conv_prob(trans, start=START, conv=CONV, null=NULL, max_steps=10_000):\n",
    "    # simulate many walks to estimate conversion probability\n",
    "    rng = np.random.default_rng(0)\n",
    "    convs = 0\n",
    "    N = 5000\n",
    "    states = list(trans.keys())\n",
    "    for _ in range(N):\n",
    "        s = start\n",
    "        steps = 0\n",
    "        while s not in (conv, null) and steps < 50:\n",
    "            probs = trans.get(s, {})\n",
    "            if not probs:\n",
    "                s = null\n",
    "                break\n",
    "            next_states = list(probs.keys())\n",
    "            p = np.array([probs[t] for t in next_states])\n",
    "            s = rng.choice(next_states, p=p)\n",
    "            steps += 1\n",
    "        if s == conv:\n",
    "            convs += 1\n",
    "    return convs / N\n",
    "\n",
    "paths = build_paths(journeys)\n",
    "T_full = transition_matrix(paths)\n",
    "p_full = conv_prob(T_full)\n",
    "p_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a91718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removal effect: drop a channel from paths and recompute conversion probability\n",
    "def remove_channel_from_paths(paths, ch):\n",
    "    new_paths = []\n",
    "    for path in paths:\n",
    "        new_path = [s for s in path if s != ch]\n",
    "        # ensure start then end state exists\n",
    "        if new_path[0] != \"_start_\":\n",
    "            new_path = [START] + new_path\n",
    "        if new_path[-1] not in (\"_conv_\",\"_null_\"):\n",
    "            # if channel removal leaves end ambiguous, append original end state\n",
    "            if path[-1] in (\"_conv_\",\"_null_\"):\n",
    "                new_path.append(path[-1])\n",
    "            else:\n",
    "                new_path.append(\"_null_\")\n",
    "        new_paths.append(new_path)\n",
    "    return new_paths\n",
    "\n",
    "removal_effect = {}\n",
    "for ch in channels:\n",
    "    T_drop = transition_matrix(remove_channel_from_paths(paths, ch))\n",
    "    p_drop = conv_prob(T_drop)\n",
    "    removal_effect[ch] = max(p_full - p_drop, 0.0)\n",
    "\n",
    "pd.Series(removal_effect).sort_values(ascending=False).rename('markov_removal_effect').to_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a586eb",
   "metadata": {},
   "source": [
    "### Compare Views: Heuristics vs. Logistic vs. Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52990230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'last_touch': baseline['last_touch_credit'],\n",
    "    'time_decay': baseline['time_decay_credit']\n",
    "}).reindex(channels).fillna(0)\n",
    "\n",
    "comparison['coef_score'] = comparison.index.map(lambda ch: channel_scores.get(ch, 0.0))\n",
    "comparison['markov_removal'] = comparison.index.map(lambda ch: removal_effect.get(ch, 0.0))\n",
    "\n",
    "# Normalize each column to sum to 1 for comparability\n",
    "def normalize(s):\n",
    "    s = s.clip(lower=0)\n",
    "    tot = s.sum()\n",
    "    return s / tot if tot > 0 else s\n",
    "\n",
    "for col in comparison.columns:\n",
    "    comparison[col] = normalize(comparison[col])\n",
    "\n",
    "comparison.round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b397778",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Practical Guidance\n",
    "- Use **heuristics** as operational baselines only.  \n",
    "- Prefer a **regularized model** (logistic, gradient boosting) with **time windows** and **sequence features**.  \n",
    "- Validate directional insights via **geo‑experiments** or **audience holdouts** before shifting budgets.\n",
    "\n",
    "### References (non‑link citations)\n",
    "1. Dalessandro, Perlich & Provost — *Causally Motivated Attribution for Online Advertising*.  \n",
    "2. Shao & Li — *Data‑Driven Multi‑Touch Attribution Models*.  \n",
    "3. Greene — *Econometric Analysis*.  \n",
    "4. McCarthy & Perlich — *Modern Approaches to Media Mix Modeling* (contextual complement to MTA).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
