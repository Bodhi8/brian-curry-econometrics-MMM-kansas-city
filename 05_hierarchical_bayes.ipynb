{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540fabef",
   "metadata": {},
   "source": [
    "\n",
    "# Hierarchical Bayes (HB) — Beta‑Binomial Partial Pooling — Python Notebook\n",
    "\n",
    "**When to Use**  \n",
    "- You need **entity‑level estimates** (stores, geos, audiences, creatives) but each entity has **limited data**.  \n",
    "- Want to **borrow strength** across entities while preserving heterogeneity.  \n",
    "- Estimating **conversion rates**, **response probabilities**, or **success proportions** with uncertainty.\n",
    "\n",
    "**Best Application**  \n",
    "- Funnel metrics (CTR, CVR) at geo/store/audience level.  \n",
    "- Small‑sample segments where stand‑alone MLEs are unstable.  \n",
    "- Prioritization decisions (which geos to scale) with **posterior intervals**.\n",
    "\n",
    "**When Not to Use**  \n",
    "- When simple pooled model is sufficient (homogeneous entities) or when you need **strict frequentist inference** only.  \n",
    "- If the goal is **causal effect estimation** under selection—use causal inference (IV/PSM/RCT) instead.\n",
    "\n",
    "**How to Interpret Results**  \n",
    "- **Posterior means** for each entity are **shrunk** toward the global mean; the amount of shrinkage depends on sample size and variance.  \n",
    "- **Hyperparameters (α, β)** reflect the global prior over entity‑level probabilities.  \n",
    "- Use **credible intervals** to compare entities and to make **risk‑aware** decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cde02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import gammaln\n",
    "from dataclasses import dataclass\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "rng = np.random.default_rng(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff954c9d",
   "metadata": {},
   "source": [
    "### Data: Synthetic store‑level conversions (Beta‑Binomial ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_stores = 40\n",
    "# True global prior for store conversion rates\n",
    "alpha_true, beta_true = 2.5, 8.0\n",
    "theta_true = rng.beta(alpha_true, beta_true, size=n_stores)\n",
    "\n",
    "trials = rng.integers(60, 160, size=n_stores)      # impressions per store (toy)\n",
    "conversions = rng.binomial(trials, theta_true)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'store_id': np.arange(n_stores),\n",
    "    'impressions': trials,\n",
    "    'conversions': conversions,\n",
    "    'ctr_empirical': conversions / trials\n",
    "})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4d9a5",
   "metadata": {},
   "source": [
    "### Model: Beta‑Binomial with hyperpriors and MCMC (Metropolis‑within‑Gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36245e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Likelihood:\n",
    "# k_i | theta_i ~ Binomial(n_i, theta_i)\n",
    "# theta_i | alpha, beta ~ Beta(alpha, beta)\n",
    "# Hyperpriors: alpha ~ Gamma(a0, b0), beta ~ Gamma(a0, b0)  (shape-rate)\n",
    "a0, b0 = 0.5, 0.5\n",
    "\n",
    "def log_beta(a, b):\n",
    "    return gammaln(a) + gammaln(b) - gammaln(a+b)\n",
    "\n",
    "def log_p_alpha_beta(alpha, beta, k, n):\n",
    "    # log posterior up to constant: sum_i log Beta(alpha+k_i, beta+n_i-k_i) - log Beta(alpha,beta)\n",
    "    # + log Gamma prior\n",
    "    if alpha <= 0 or beta <= 0:\n",
    "        return -np.inf\n",
    "    term = np.sum(log_beta(alpha + k, beta + n - k)) - n.size * log_beta(alpha, beta)\n",
    "    # Gamma(a0,b0): log prior = (a0-1)log(x) - b0 x + const\n",
    "    term += (a0-1)*np.log(alpha) - b0*alpha + (a0-1)*np.log(beta) - b0*beta\n",
    "    return float(term)\n",
    "\n",
    "@dataclass\n",
    "class HBState:\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    theta: np.ndarray\n",
    "\n",
    "def sample_thetas(alpha, beta, k, n, rng):\n",
    "    # theta_i | rest ~ Beta(alpha+k_i, beta + n_i - k_i) (conjugate)\n",
    "    return rng.beta(alpha + k, beta + n - k)\n",
    "\n",
    "def mh_step_alpha_beta(state, k, n, step=0.25, rng=np.random.default_rng()):\n",
    "    # Random walk on log scale for alpha and beta\n",
    "    a_cur, b_cur = state.alpha, state.beta\n",
    "    la_cur, lb_cur = np.log(a_cur), np.log(b_cur)\n",
    "    la_prop = la_cur + rng.normal(0, step)\n",
    "    lb_prop = lb_cur + rng.normal(0, step)\n",
    "    a_prop, b_prop = np.exp(la_prop), np.exp(lb_prop)\n",
    "\n",
    "    lp_cur = log_p_alpha_beta(a_cur, b_cur, k, n)\n",
    "    lp_prop = log_p_alpha_beta(a_prop, b_prop, k, n)\n",
    "\n",
    "    # Jacobian terms for log-transform proposals cancel in ratio if symmetric; accept/reject\n",
    "    if np.log(rng.random()) < (lp_prop - lp_cur):\n",
    "        return a_prop, b_prop, True\n",
    "    else:\n",
    "        return a_cur, b_cur, False\n",
    "\n",
    "def run_mcmc(k, n, iters=4000, burn=1500, step=0.25, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Initialize with method-of-moments\n",
    "    p_hat = k.sum() / n.sum()\n",
    "    v_hat = np.var(k/n)\n",
    "    # Rough init for alpha,beta\n",
    "    s = 50 * p_hat * (1-p_hat) / max(v_hat, 1e-3)\n",
    "    alpha = max(0.1, p_hat * s)\n",
    "    beta = max(0.1, (1-p_hat) * s)\n",
    "    theta = sample_thetas(alpha, beta, k, n, rng)\n",
    "\n",
    "    trace_alpha, trace_beta = [], []\n",
    "    accept = 0\n",
    "    for t in range(iters):\n",
    "        # Update thetas (conjugate)\n",
    "        theta = sample_thetas(alpha, beta, k, n, rng)\n",
    "        # Update alpha,beta via MH\n",
    "        alpha, beta, acc = mh_step_alpha_beta(HBState(alpha, beta, theta), k, n, step=step, rng=rng)\n",
    "        accept += int(acc)\n",
    "        trace_alpha.append(alpha)\n",
    "        trace_beta.append(beta)\n",
    "    trace = pd.DataFrame({'alpha': trace_alpha, 'beta': trace_beta})\n",
    "    post = trace.iloc[burn:].reset_index(drop=True)\n",
    "    return post, theta, accept/iters\n",
    "\n",
    "k = df['conversions'].values\n",
    "n = df['impressions'].values\n",
    "post, theta_last, acc_rate = run_mcmc(k, n, iters=5000, burn=2000, step=0.20, seed=9)\n",
    "acc_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc555c7",
   "metadata": {},
   "source": [
    "### Posterior for Hyperparameters α, β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_summary = post[['alpha','beta']].agg(['mean','std','median','quantile'])\n",
    "post_summary.loc['q05'] = post[['alpha','beta']].quantile(0.05)\n",
    "post_summary.loc['q95'] = post[['alpha','beta']].quantile(0.95)\n",
    "post_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(post['alpha'], bins=40, alpha=0.6, label='alpha')\n",
    "ax.hist(post['beta'], bins=40, alpha=0.6, label='beta')\n",
    "ax.set_title('Posterior of alpha and beta')\n",
    "ax.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812c98a",
   "metadata": {},
   "source": [
    "### Entity‑level Posteriors and Shrinkage vs. Raw Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Posterior mean for each theta_i under Beta(alpha+ki, beta+ni-ki) averaged over posterior draws of alpha,beta\n",
    "# For speed, approximate using posterior means of alpha,beta\n",
    "alpha_hat = post['alpha'].mean()\n",
    "beta_hat  = post['beta'].mean()\n",
    "\n",
    "df['theta_posterior_mean'] = (df['conversions'] + alpha_hat) / (df['impressions'] + alpha_hat + beta_hat)\n",
    "\n",
    "# Compare to empirical CTR\n",
    "plt.scatter(df['impressions'], df['ctr_empirical'], label='Empirical (MLE)', alpha=0.7)\n",
    "plt.scatter(df['impressions'], df['theta_posterior_mean'], label='HB Posterior Mean', alpha=0.7)\n",
    "plt.axhline((alpha_hat)/(alpha_hat+beta_hat), color='k', linestyle='--', label='Global mean')\n",
    "plt.xlabel('Impressions (sample size)')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Shrinkage: HB vs Empirical Rates')\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "df[['store_id','impressions','conversions','ctr_empirical','theta_posterior_mean']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62c01e",
   "metadata": {},
   "source": [
    "### Ranking Stores with Uncertainty (Posterior Intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38941ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute posterior intervals by sampling theta_i conditional on alpha_hat, beta_hat (approximation)\n",
    "S = 2000\n",
    "theta_samples = np.zeros((len(df), S))\n",
    "for s in range(S):\n",
    "    theta_samples[:, s] = rng.beta(alpha_hat + df['conversions'].values,\n",
    "                                   beta_hat + df['impressions'].values - df['conversions'].values)\n",
    "low = np.quantile(theta_samples, 0.05, axis=1)\n",
    "high = np.quantile(theta_samples, 0.95, axis=1)\n",
    "df['theta_low95'] = low\n",
    "df['theta_high95'] = high\n",
    "\n",
    "top = df.sort_values('theta_posterior_mean', ascending=False).head(10)\n",
    "top[['store_id','theta_posterior_mean','theta_low95','theta_high95']].round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize top 10 with intervals\n",
    "tmp = top.sort_values('theta_posterior_mean', ascending=True)\n",
    "plt.errorbar(tmp['theta_posterior_mean'], tmp['store_id'], \n",
    "             xerr=[tmp['theta_posterior_mean']-tmp['theta_low95'], \n",
    "                   tmp['theta_high95']-tmp['theta_posterior_mean']], \n",
    "             fmt='o')\n",
    "plt.xlabel('Posterior rate (95% CI)')\n",
    "plt.ylabel('Store ID')\n",
    "plt.title('Top Stores by Posterior Rate with 95% CI')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a6b8f",
   "metadata": {},
   "source": [
    "### Posterior Predictive Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split stores into train/test by masking some stores (toy PPC)\n",
    "mask = rng.random(len(df)) < 0.7\n",
    "train_df = df[mask].copy()\n",
    "test_df  = df[~mask].copy()\n",
    "\n",
    "k_tr, n_tr = train_df['conversions'].values, train_df['impressions'].values\n",
    "post_tr, _, _ = run_mcmc(k_tr, n_tr, iters=3000, burn=1500, step=0.2, seed=11)\n",
    "a_hat, b_hat = post_tr['alpha'].mean(), post_tr['beta'].mean()\n",
    "\n",
    "# Predictive distribution for held-out stores' conversions given impressions\n",
    "S = 1000\n",
    "pred_means, obs_rates = [], []\n",
    "for _, row in test_df.iterrows():\n",
    "    n_i = int(row['impressions'])\n",
    "    # sample theta ~ Beta(a_hat,b_hat), then k ~ Binomial(n, theta)\n",
    "    thetas = rng.beta(a_hat, b_hat, size=S)\n",
    "    ks = rng.binomial(n_i, thetas)\n",
    "    pred_means.append(ks.mean()/n_i)\n",
    "    obs_rates.append(row['conversions']/n_i)\n",
    "\n",
    "ppc_df = pd.DataFrame({'obs_rate': obs_rates, 'pred_mean_rate': pred_means})\n",
    "ppc_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b187fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(ppc_df['obs_rate'], ppc_df['pred_mean_rate'])\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('Observed Rate (Test)')\n",
    "plt.ylabel('Posterior Predictive Mean Rate')\n",
    "plt.title('Posterior Predictive Calibration (Held-out Stores)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d9793",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Practical Guidance\n",
    "- HB provides **robust small‑sample estimates** by shrinking extreme MLEs toward a global prior.  \n",
    "- Use **entity‑level intervals** for risk‑aware ranking and to avoid over‑reacting to noise.  \n",
    "- Extend to **covariates** by modeling logit(theta_i) with a hierarchical regression (requires MCMC/VI framework).\n",
    "\n",
    "### References (non‑link citations)\n",
    "1. Rossi, Allenby & McCulloch — *Bayesian Statistics and Marketing*.  \n",
    "2. Gelman et al. — *Bayesian Data Analysis*.  \n",
    "3. Fader & Hardie — *Probability Models for Customer‑Base Analysis*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
